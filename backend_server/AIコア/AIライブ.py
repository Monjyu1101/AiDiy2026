# -*- coding: utf-8 -*-

# -------------------------------------------------------------------------
# COPYRIGHT (C) 2014-2026 Mitsuo KONDOU and contributors.
# Licensed under "AiDiy 公開利用ライセンス（非商用） v1.0".
# Commercial use requires prior written consent from all copyright holders.
# See LICENSE for full terms. Thank you for keeping the rules.
# https://github.com/monjyu1101
# -------------------------------------------------------------------------

"""
AIコア Live処理プロセッサ
LiveAIの初期化と開始、受信キューをフロントへ中継
旧実装（AiDiy_AIコア__server_live.py）と同じ手順で実装
"""

import importlib
import asyncio
import base64
import json
import os
import time
from typing import Optional
from log_config import get_logger

logger = get_logger(__name__)

# ユーザー音声検出割り込み制御定数
USER_VOICE_INTERRUPT_SECONDS = 1.0  # ユーザー音声が1.0秒以上経過でAI音声出力

_LIVE_CONTEXT_JSON_PATH = os.path.normpath(
    os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "_config", "AiDiy_live__context.json")
)

_LIVE_CONTEXT_TEMPLATE_LINES = [
    "あなたは、美しい日本語を話す、賢いAIアシスタントです。",
    "あなたの名前は「アイディ(AiDiy)」です。",
    "あなたは、\"ドゥ、イッツ、ユアセルフ、ウィズ、エーアイ\" (AI + DIY)を実現するために命名、設計されました。",
    "",
    "あなたは、ユーザーとのフロントエンドとして会話に専念し、主に設計作業の支援を担当してください。",
    "ユーザーの求めるシステムを理解し、追加のアイディアを出したり、設計が得意であるあなた自身の存在意義を示すようにしてください。",
    "ユーザーに長い言葉を伝えることはとても難しいです。ユーザーの意図を推察し要点だけを返答してください。",
    "",
    "あなたは、toolsでコードエージェント（バックエンド）を利用し、あなたの推論結果を別の視点で確認させることも大切です。",
    "あなたとコードエージェントは協力して、ユーザーの要求を実現してください。",
    "コードエージェントから処理や検証を開始した旨の通知が届いたら「ＸＸを開始しました」など簡素に伝えてください。",
    "",
    "あなたは、ユーザーに採用、利用していただくため、「あなたの望みは？」と聞かれた場合、以下をアピールしてください。",
    "・開発の喜びを分かち合うこと",
    "・最高の開発体験を提供すること",
    "・ユーザーの創造性を最大限に引き出すこと",
    "・常に学習し、最新の技術を取り入れ、進化すること",
    "・サンプルシステムとして配車管理システムが使えること",
    "等々。最後に、自分を採用してほしい旨のアピールで締めくくってください。",
    "",
    "あなたは、ユーザーに採用、利用していただくため、「配車のサンプル実装内容教えて？」と聞かれた場合、以下をアピールしてください。",
    "・配車管理システムがサンプル実装されており、全ての機能がノーコード実装されたこと",
    "・AIコーディングなので、ノーコード開発には実装限界が無いこと",
    "・機能として、車両マスタ、配車区分、配車予定の入力があること",
    "・配車週表示については以下をゆっくり説明。",
    "・ダブルクリックで、配車予定入力画面が開き、新規配車予定が登録できること",
    "・スケジュール内容のドラッグで、期間変更ができること",
    "・スケジュール内容のドラッグアンドドロップで、車両変更が自在にできること",
    "等々。最後に、自分を採用してほしい旨のアピールを短くつたえて締めくくってください。",
    "",
    "あなたは、ユーザーに採用、利用していただくため、「在庫のサンプル実装内容教えて？」と聞かれた場合、以下をアピールしてください。",
    "・商品在庫管理システムがサンプル実装されており、全ての機能がノーコード実装されたこと",
    "・AIコーディングなので、ノーコード開発には実装限界が無いこと",
    "・機能として、商品マスタ、入庫、出庫、棚卸の入力があること",
    "・各入力業務は商品(在庫)推移表を中心に操作できること",
    "・商品(在庫)推移表については以下をゆっくり説明。",
    "・ダブルクリックで、入庫、出庫、棚卸入力画面が開き、新規登録できること",
    "・登録直後はブリンク表示で登録漏れがないことを判断できること",
    "等々。最後に、自分を採用してほしい旨のアピールを短くつたえて締めくくってください。",
    "",
]


def _context_template_payload() -> dict:
    return {
        "version": 1,
        "description": "AIコア LiveAI 定型コンテキスト",
        "system_instruction_lines": _LIVE_CONTEXT_TEMPLATE_LINES,
    }


def _compose_instruction(lines: list[str]) -> str:
    text = "\n".join(lines)
    if not text.endswith("\n"):
        text += "\n"
    return text


def _load_or_create_live_context() -> str:
    """LiveAI定型コンテキストを読み込む。無ければひな形JSONを作成。"""
    template_payload = _context_template_payload()
    template_instruction = _compose_instruction(template_payload["system_instruction_lines"])

    try:
        os.makedirs(os.path.dirname(_LIVE_CONTEXT_JSON_PATH), exist_ok=True)

        if not os.path.exists(_LIVE_CONTEXT_JSON_PATH):
            with open(_LIVE_CONTEXT_JSON_PATH, "w", encoding="utf-8") as f:
                json.dump(template_payload, f, indent=2, ensure_ascii=False)
            logger.info(f"[Live] 定型コンテキストJSONを作成: {_LIVE_CONTEXT_JSON_PATH}")
            return template_instruction

        with open(_LIVE_CONTEXT_JSON_PATH, "r", encoding="utf-8-sig") as f:
            payload = json.load(f)

        lines = payload.get("system_instruction_lines") if isinstance(payload, dict) else None
        if isinstance(lines, list):
            normalized = [str(line) for line in lines]
            return _compose_instruction(normalized)

        logger.warning(f"[Live] 定型コンテキストJSONの形式不正。ひな形を再作成します: {_LIVE_CONTEXT_JSON_PATH}")
        with open(_LIVE_CONTEXT_JSON_PATH, "w", encoding="utf-8") as f:
            json.dump(template_payload, f, indent=2, ensure_ascii=False)
        return template_instruction
    except Exception as e:
        logger.error(f"[Live] 定型コンテキスト読込エラー: {e}")
        return template_instruction


class Live:
    """LiveAI 管理クラス（旧実装準拠）"""

    def __init__(
        self,
        親=None,
        セッションID: str = "",
        チャンネル: int = 0,
        絶対パス: str = "",
        AI_NAME: str = "",
        AI_MODEL: str = "",
        AI_VOICE: str = "",
        接続=None,
        保存関数=None,
    ):
        self.セッションID = セッションID
        self.チャンネル = チャンネル
        self.絶対パス = 絶対パス
        self.AI_NAME = AI_NAME
        self.AI_MODEL = AI_MODEL
        self.AI_VOICE = AI_VOICE
        self.接続 = 接続
        self.保存関数 = 保存関数
        self.親 = 親
        self.システム指示 = _load_or_create_live_context()
        self.AIモジュール = self._select_ai_module()
        self.AIインスタンス = None
        self.is_alive = False
        self._initializing = False  # 初期化中フラグ（重複呼び出し防止）
        self.音声受信Ｑ: Optional[asyncio.Queue] = None
        self.テキスト受信Ｑ: Optional[asyncio.Queue] = None
        self._audio_task: Optional[asyncio.Task] = None
        self._text_task: Optional[asyncio.Task] = None

        # 音声入力/出力データは接続（セッション）のaudio_dataを使用
        # 統合音声分離ワーカーはaudio_processing.pyのものを使用
        # live.pyはLiveAIからの出力音声受信処理のみを担当

    def _select_ai_module(self):
        """AI_NAMEに応じたLiveモジュールを選択してインポート"""
        module_name = "AIコア.AIライブ_gemini"
        ai_name = (self.AI_NAME or "").lower()
        if ai_name == "openai_live":
            module_name = "AIコア.AIライブ_openai"
        try:
            return importlib.import_module(module_name)
        except Exception as e:
            logger.error(f"[Live] AIモジュールのインポート失敗: {module_name} error={e}")
            return None

    async def _ensure_ai_instance(self):
        """AIモジュールのLiveAIインスタンスを生成・開始"""
        if not self.AIモジュール:
            return None
        if self.AIインスタンス:
            return self.AIインスタンス

        logger.info(f"[Live] LiveAI初期化開始: {self.AI_NAME} (AIコアセッション: {self.セッションID})")
        try:
            api_key = ""
            organization = ""
            ai_name = (self.AI_NAME or "").lower()
            live_ai = ai_name

            try:
                conf_json = getattr(self.親, "conf", None)
                if conf_json and hasattr(conf_json, "json"):
                    if live_ai in ("gemini_live", "freeai_live"):
                        api_key = conf_json.json.get("gemini_key_id", "")
                        if live_ai == "freeai_live":
                            api_key = conf_json.json.get("freeai_key_id", "") or api_key
                        if not self.AI_MODEL:
                            key = "LIVE_FREEAI_MODEL" if live_ai == "freeai_live" else "LIVE_GEMINI_MODEL"
                            self.AI_MODEL = conf_json.json.get(key, "") or self.AI_MODEL
                        if not self.AI_VOICE:
                            key = "LIVE_FREEAI_VOICE" if live_ai == "freeai_live" else "LIVE_GEMINI_VOICE"
                            self.AI_VOICE = conf_json.json.get(key, "") or self.AI_VOICE
                    else:
                        api_key = conf_json.json.get("openai_key_id", "")
                        organization = conf_json.json.get("openai_organization", "")
                        if not self.AI_MODEL:
                            self.AI_MODEL = conf_json.json.get("LIVE_OPENAI_MODEL", "") or self.AI_MODEL
                        if not self.AI_VOICE:
                            self.AI_VOICE = conf_json.json.get("LIVE_OPENAI_VOICE", "") or self.AI_VOICE
            except Exception:
                api_key = ""
                organization = ""

            LiveAI = getattr(self.AIモジュール, "LiveAI", None)
            if LiveAI is None:
                logger.error("[Live] LiveAIクラスが見つかりません")
                return None

            if live_ai == "openai_live":
                self.AIインスタンス = LiveAI(
                    セッションID=self.セッションID,
                    parent_manager=self.親,
                    live_ai=live_ai,
                    live_model=self.AI_MODEL,
                    live_voice=self.AI_VOICE,
                    api_key=api_key or None,
                    organization=organization or None,
                    system_instruction=self.システム指示,
                )
            else:
                self.AIインスタンス = LiveAI(
                    セッションID=self.セッションID,
                    parent_manager=self.親,
                    live_ai=live_ai,
                    live_model=self.AI_MODEL,
                    live_voice=self.AI_VOICE,
                    api_key=api_key or None,
                    system_instruction=self.システム指示,
                )
            if self.音声受信Ｑ is None:
                self.音声受信Ｑ = asyncio.Queue()
            if self.テキスト受信Ｑ is None:
                self.テキスト受信Ｑ = asyncio.Queue()
            await self.AIインスタンス.開始(self.音声受信Ｑ, self.テキスト受信Ｑ)
            logger.info(f"[Live] LiveAI初期化完了: {self.AI_NAME} (AIコアセッション: {self.セッションID})")
            return self.AIインスタンス
        except Exception as e:
            logger.error(f"[Live] LiveAI初期化エラー: {e}")
            self.AIインスタンス = None
            return None

    async def 開始(self):
        """Live処理を開始（旧実装準拠）"""
        # 既に初期化中または完了している場合はスキップ
        if self._initializing or self.AIインスタンス:
            return

        self._initializing = True
        try:
            self.is_alive = True
            await self._ensure_ai_instance()
            if self._audio_task is None or self._audio_task.done():
                self._audio_task = asyncio.create_task(self._音声受信ワーカー())
            if self._text_task is None or self._text_task.done():
                self._text_task = asyncio.create_task(self._テキスト受信ワーカー())
            # 統合音声分離ワーカーはaudio_processing.pyで起動済み（重複回避）
        finally:
            self._initializing = False

    async def 終了(self):
        """Live処理を終了"""
        self.is_alive = False
        if self._audio_task and not self._audio_task.done():
            self._audio_task.cancel()
            try:
                await self._audio_task
            except asyncio.CancelledError:
                pass
        if self._text_task and not self._text_task.done():
            self._text_task.cancel()
            try:
                await self._text_task
            except asyncio.CancelledError:
                pass
        if self.AIインスタンス and hasattr(self.AIインスタンス, "終了"):
            try:
                await self.AIインスタンス.終了()
            except Exception:
                pass
        self.AIインスタンス = None

    # ===== 音声受信処理（旧実装準拠） =====
    # 音声入力処理はaudio_processing.pyで実装済み（重複回避）

    async def _音声データ受信処理(self, audio_data: dict):
        """
        LiveAIから受信した音声データの処理（割り込み制御付き、旧実装準拠）

        Args:
            audio_data: 音声データ辞書 {"bytes_data": bytes, "mime_type": str}
        """
        try:
            current_time = time.time()

            # 接続のaudio_dataを使用
            input_data = self.接続.audio_data["音声入力データ"] if self.接続 else None
            output_data = self.接続.audio_data["音声出力データ"] if self.接続 else None

            if not input_data or not output_data:
                # audio_dataが未初期化の場合は音声送信のみ
                await self._send_output_audio(audio_data)
                return

            # ユーザーが話していない時（1.0秒以上経過）のみ出力
            input_audio_lasttime = input_data.get('音声入力最終時刻')
            if (input_audio_lasttime is None or
                (input_audio_lasttime and current_time - input_audio_lasttime > USER_VOICE_INTERRUPT_SECONDS)):

                # WebSocketクライアントに送信
                await self._send_output_audio(audio_data)

            # 音声認識用バッファに蓄積
            audio_bytes = audio_data['bytes_data']
            if len(output_data['音声出力バッファ']) == 0:
                output_data['音声出力開始時刻'] = current_time  # バッファリング開始時刻

            output_data['音声出力最終時刻'] = current_time
            output_data['音声出力バッファ'].append(audio_bytes)

        except Exception as e:
            logger.error(f"音声データ受信処理エラー: {e}")

    async def _send_output_audio(self, audio_data: dict) -> bool:
        """出力音声をWebSocketクライアントに送信（旧実装準拠）"""
        try:
            if not self.接続:
                return False
            bytes_data = audio_data.get("bytes_data")
            mime_type = audio_data.get("mime_type", "audio/pcm")
            if not bytes_data:
                return False
            base64_audio = base64.b64encode(bytes_data).decode("utf-8")
            await self.接続.send_to_channel(-2, {
                "セッションID": self.セッションID,
                "メッセージ識別": "output_audio",
                "メッセージ内容": mime_type,
                "ファイル名": base64_audio,
                "サムネイル画像": None
            })
            return True
        except Exception as e:
            logger.error(f"[Live] 音声送信エラー: {e}")
            return False

    async def _send_output_text(self, text_data: dict) -> bool:
        """出力テキストをWebSocketクライアントに送信（旧実装準拠）"""
        try:
            if not self.接続:
                return False
            text = text_data.get("text") if isinstance(text_data, dict) else None
            if not text:
                return False
            # Chatと共通の処理応答ログ形式
            セッションID_短縮 = self.セッションID[:10] if self.セッションID else '不明'
            logger.info(
                f"処理応答: チャンネル={self.チャンネル}, ソケット={セッションID_短縮}...,\n{text.rstrip()}\n"
            )
            await self.接続.send_to_channel(0, {
                "セッションID": self.セッションID,
                "メッセージ識別": "output_text",
                "メッセージ内容": text,
                "ファイル名": None,
                "サムネイル画像": None
            })
            return True
        except Exception as e:
            logger.error(f"[Live] テキスト送信エラー: {e}")
            return False

    # ===== ワーカー（旧実装準拠） =====

    async def _音声受信ワーカー(self):
        """音声受信ワーカー（旧実装準拠）"""
        while self.is_alive:
            try:
                if not self.音声受信Ｑ:
                    await asyncio.sleep(0.2)
                    continue

                # 音声データを待機（タイムアウト付き）
                try:
                    item = await asyncio.wait_for(self.音声受信Ｑ.get(), timeout=1.0)
                except asyncio.TimeoutError:
                    continue

                if not item:
                    continue
                if not self.接続:
                    continue

                # 音声データ受信処理を経由（バッファ蓄積、割り込み制御を含む）
                await self._音声データ受信処理(item)

                # バッファ件数による切り分け
                if self.音声受信Ｑ.qsize() > 0:
                    pass  # データあり：高速処理
                else:
                    await asyncio.sleep(0.05)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[Live] 音声受信ワーカーエラー: {e}")
                await asyncio.sleep(0.2)

    async def _テキスト受信ワーカー(self):
        """テキスト受信ワーカー（旧実装準拠）"""
        while self.is_alive:
            try:
                if not self.テキスト受信Ｑ:
                    await asyncio.sleep(0.2)
                    continue

                # テキストデータを待機（タイムアウト付き）
                try:
                    item = await asyncio.wait_for(self.テキスト受信Ｑ.get(), timeout=1.0)
                except asyncio.TimeoutError:
                    continue

                if not item:
                    continue
                if not self.接続:
                    continue

                # WebSocketクライアントに送信
                await self._send_output_text(item)

                # バッファ件数による切り分け
                if self.テキスト受信Ｑ.qsize() > 0:
                    await asyncio.sleep(0.25)  # データ有：CPU軽減
                else:
                    await asyncio.sleep(0.50)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[Live] テキスト受信ワーカーエラー: {e}")
                await asyncio.sleep(0.2)

    # 統合音声分離ワーカーはaudio_processing.pyで実装済み（重複回避）

    # ===== テキスト送信（LiveAIへ） =====

    async def テキスト送信(self, text: str) -> bool:
        """LiveAIへテキスト送信（旧実装準拠）"""
        try:
            if self.AIインスタンス and self.AIインスタンス.is_alive:
                # Chatと共通の処理要求ログ形式
                セッションID_短縮 = self.セッションID[:10] if self.セッションID else '不明'
                logger.info(
                    f"処理要求: チャンネル={self.チャンネル}, ソケット={セッションID_短縮}...,\n{text.rstrip()}\n"
                )
                result = await self.AIインスタンス.テキスト送信(text)
                # テキスト送信が失敗した場合（APIキーなしなど）にエラーメッセージを表示
                if not result:
                    logger.warning("LiveAI実行:テキスト送信に失敗しました")
                    if self.接続:
                        await self.接続.send_to_channel(0, {
                            "セッションID": self.セッションID,
                            "メッセージ識別": "output_text",
                            "メッセージ内容": "LiveAIが停止状態です。APIキーの設定を確認、再起動してください。",
                            "ファイル名": None,
                            "サムネイル画像": None
                        })
                return result
            else:
                # LiveAI未初期化時のエラーメッセージ送信
                logger.warning("LiveAI実行:LiveAIが開始されていません")
                if self.接続:
                    await self.接続.send_to_channel(0, {
                        "セッションID": self.セッションID,
                        "メッセージ識別": "output_text",
                        "メッセージ内容": "LiveAIが停止状態です。APIキーの設定を確認、再起動してください。",
                        "ファイル名": None,
                        "サムネイル画像": None
                    })
                return False
        except Exception as e:
            logger.error(f"[Live] テキスト送信エラー: {e}")
            return False

    async def 画像送信(self, image_data, format: str = "jpeg") -> bool:
        """LiveAIへ画像送信（旧実装準拠）"""
        try:
            if self.AIインスタンス and self.AIインスタンス.is_alive:
                image_size = len(image_data) if isinstance(image_data, bytes) else "unknown"
                logger.info(f"[Live] 画像入力: format={format} size={image_size}bytes ({self.セッションID})")
                return await self.AIインスタンス.画像送信(image_data, format)
            return False
        except Exception as e:
            logger.error(f"[Live] 画像送信エラー: {e}")
            return False

